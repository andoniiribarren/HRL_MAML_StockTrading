{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d915aa32",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d686209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fc9d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (45900, 8)\n"
     ]
    }
   ],
   "source": [
    "from finHRL.preprocess.preprocessor import YahooDownloader\n",
    "\n",
    "with open(\"../finHRL/preprocess/tickers/ticker_lists.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dow_30 = data[\"DOW_30\"]\n",
    "cryptos = data[\"CRYPTO_7\"]\n",
    "\n",
    "TRAIN_START_DATE = '2017-01-01'\n",
    "TRAIN_END_DATE = '2022-01-01'\n",
    "TEST_START_DATE = '2022-01-01'\n",
    "TEST_END_DATE = '2023-01-01'\n",
    "\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = pd.to_datetime(TRAIN_START_DATE) - datetime.timedelta(days=30),\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = dow_30).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0b6be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = ['macd',\n",
    "              'rsi_30',\n",
    "              'cci_30']\n",
    "\n",
    "from finHRL.preprocess.preprocessor import FeatureEngineer\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=False,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)\n",
    "\n",
    "processed = processed[processed.date >= TRAIN_START_DATE].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8417959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "print(stock_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c7441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = processed[processed.date < TEST_START_DATE]\n",
    "df_test = processed[processed.date >= TEST_START_DATE]\n",
    "\n",
    "\n",
    "df_train[\"dayorder\"] = df_train[\"date\"].astype(\"category\").cat.codes\n",
    "df_test[\"dayorder\"] = df_test[\"date\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca4d0d",
   "metadata": {},
   "source": [
    "# Base RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7fc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finHRL.env_stocktrading.trading_env_RL import StockTradingEnv\n",
    "\n",
    "# state_space_noHRL = [balance, close prices_i, stock_shares_i, MACD_i, rsi30_i, cci30_i, turbulences_i]\n",
    "episode_len = df_train.dayorder.nunique()\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension \n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "tr_env = StockTradingEnv(\n",
    "    df = df_train,\n",
    "    stock_dim=stock_dimension,\n",
    "    hmax= 100,\n",
    "    initial_amount=1000000,\n",
    "    num_stock_shares=num_stock_shares,\n",
    "    buy_cost_pct=buy_cost_list,\n",
    "    sell_cost_pct=sell_cost_list,\n",
    "    state_space= state_space,\n",
    "    action_space= stock_dimension,\n",
    "    tech_indicator_list=INDICATORS,\n",
    "    make_plots=False,\n",
    "    print_verbosity=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7575caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "day: 1258, episode: 1\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2139391.32\n",
      "total_reward: 1139391.32\n",
      "total_cost: 236364.09\n",
      "total_trades: 35236\n",
      "Sharpe: 0.877\n",
      "=================================\n",
      "day: 1258, episode: 2\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1929656.16\n",
      "total_reward: 929656.16\n",
      "total_cost: 230435.41\n",
      "total_trades: 35045\n",
      "Sharpe: 0.756\n",
      "=================================\n",
      "day: 1258, episode: 3\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2173411.25\n",
      "total_reward: 1173411.25\n",
      "total_cost: 230242.03\n",
      "total_trades: 35043\n",
      "Sharpe: 0.863\n",
      "=================================\n",
      "day: 1258, episode: 4\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2242601.33\n",
      "total_reward: 1242601.33\n",
      "total_cost: 228796.98\n",
      "total_trades: 34984\n",
      "Sharpe: 0.865\n",
      "=================================\n",
      "day: 1258, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1905364.98\n",
      "total_reward: 905364.98\n",
      "total_cost: 233622.92\n",
      "total_trades: 35260\n",
      "Sharpe: 0.747\n",
      "=================================\n",
      "day: 1258, episode: 6\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1999131.77\n",
      "total_reward: 999131.77\n",
      "total_cost: 226128.09\n",
      "total_trades: 34830\n",
      "Sharpe: 0.818\n",
      "=================================\n",
      "day: 1258, episode: 7\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1834171.46\n",
      "total_reward: 834171.46\n",
      "total_cost: 229404.35\n",
      "total_trades: 35058\n",
      "Sharpe: 0.726\n",
      "=================================\n",
      "day: 1258, episode: 8\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1747898.65\n",
      "total_reward: 747898.65\n",
      "total_cost: 218768.66\n",
      "total_trades: 34463\n",
      "Sharpe: 0.651\n",
      "=================================\n",
      "day: 1258, episode: 9\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1712335.96\n",
      "total_reward: 712335.96\n",
      "total_cost: 227951.72\n",
      "total_trades: 34871\n",
      "Sharpe: 0.639\n",
      "=================================\n",
      "day: 1258, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2048889.51\n",
      "total_reward: 1048889.51\n",
      "total_cost: 223754.38\n",
      "total_trades: 34756\n",
      "Sharpe: 0.780\n",
      "=================================\n",
      "day: 1258, episode: 11\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1702564.43\n",
      "total_reward: 702564.43\n",
      "total_cost: 223745.02\n",
      "total_trades: 34887\n",
      "Sharpe: 0.628\n",
      "=================================\n",
      "day: 1258, episode: 12\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1889786.70\n",
      "total_reward: 889786.70\n",
      "total_cost: 225744.32\n",
      "total_trades: 34701\n",
      "Sharpe: 0.728\n",
      "=================================\n",
      "day: 1258, episode: 13\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2111147.17\n",
      "total_reward: 1111147.17\n",
      "total_cost: 225717.75\n",
      "total_trades: 34760\n",
      "Sharpe: 0.831\n",
      "=================================\n",
      "day: 1258, episode: 14\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2081320.74\n",
      "total_reward: 1081320.74\n",
      "total_cost: 231481.91\n",
      "total_trades: 34953\n",
      "Sharpe: 0.854\n",
      "=================================\n",
      "day: 1258, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1908271.01\n",
      "total_reward: 908271.01\n",
      "total_cost: 231597.45\n",
      "total_trades: 35058\n",
      "Sharpe: 0.753\n",
      "=================================\n",
      "day: 1258, episode: 16\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1452789.87\n",
      "total_reward: 452789.87\n",
      "total_cost: 220078.64\n",
      "total_trades: 34454\n",
      "Sharpe: 0.453\n",
      "=================================\n",
      "day: 1258, episode: 17\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1811215.20\n",
      "total_reward: 811215.20\n",
      "total_cost: 226709.27\n",
      "total_trades: 34668\n",
      "Sharpe: 0.703\n",
      "=================================\n",
      "day: 1258, episode: 18\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2195961.16\n",
      "total_reward: 1195961.16\n",
      "total_cost: 233254.13\n",
      "total_trades: 34929\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "day: 1258, episode: 19\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1831154.94\n",
      "total_reward: 831154.94\n",
      "total_cost: 226636.15\n",
      "total_trades: 34624\n",
      "Sharpe: 0.698\n",
      "=================================\n",
      "day: 1258, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1691781.09\n",
      "total_reward: 691781.09\n",
      "total_cost: 224067.89\n",
      "total_trades: 34804\n",
      "Sharpe: 0.604\n",
      "=================================\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 0.644    |\n",
      "| time/                 |          |\n",
      "|    fps                | 116      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 220      |\n",
      "|    total_timesteps    | 25600    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -31.2    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -2.77    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.0704   |\n",
      "------------------------------------\n",
      "day: 1258, episode: 21\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1945586.62\n",
      "total_reward: 945586.62\n",
      "total_cost: 225862.60\n",
      "total_trades: 34864\n",
      "Sharpe: 0.764\n",
      "=================================\n",
      "day: 1258, episode: 22\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1968427.69\n",
      "total_reward: 968427.69\n",
      "total_cost: 233627.90\n",
      "total_trades: 35050\n",
      "Sharpe: 0.795\n",
      "=================================\n",
      "day: 1258, episode: 23\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1988891.51\n",
      "total_reward: 988891.51\n",
      "total_cost: 231899.04\n",
      "total_trades: 35028\n",
      "Sharpe: 0.810\n",
      "=================================\n",
      "day: 1258, episode: 24\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2170422.75\n",
      "total_reward: 1170422.75\n",
      "total_cost: 232572.22\n",
      "total_trades: 35121\n",
      "Sharpe: 0.865\n",
      "=================================\n",
      "day: 1258, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1829252.44\n",
      "total_reward: 829252.44\n",
      "total_cost: 234546.57\n",
      "total_trades: 35047\n",
      "Sharpe: 0.719\n",
      "=================================\n",
      "day: 1258, episode: 26\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1976751.31\n",
      "total_reward: 976751.31\n",
      "total_cost: 225268.57\n",
      "total_trades: 34818\n",
      "Sharpe: 0.786\n",
      "=================================\n",
      "day: 1258, episode: 27\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1581113.34\n",
      "total_reward: 581113.34\n",
      "total_cost: 224392.46\n",
      "total_trades: 34575\n",
      "Sharpe: 0.555\n",
      "=================================\n",
      "day: 1258, episode: 28\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1965549.15\n",
      "total_reward: 965549.15\n",
      "total_cost: 231929.01\n",
      "total_trades: 35097\n",
      "Sharpe: 0.779\n",
      "=================================\n",
      "day: 1258, episode: 29\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1922998.99\n",
      "total_reward: 922998.99\n",
      "total_cost: 225816.16\n",
      "total_trades: 34793\n",
      "Sharpe: 0.773\n",
      "=================================\n",
      "day: 1258, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2255956.94\n",
      "total_reward: 1255956.94\n",
      "total_cost: 225553.39\n",
      "total_trades: 34838\n",
      "Sharpe: 0.959\n",
      "=================================\n",
      "day: 1258, episode: 31\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1843558.28\n",
      "total_reward: 843558.28\n",
      "total_cost: 229902.26\n",
      "total_trades: 34861\n",
      "Sharpe: 0.710\n",
      "=================================\n",
      "day: 1258, episode: 32\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2010827.76\n",
      "total_reward: 1010827.76\n",
      "total_cost: 228989.15\n",
      "total_trades: 34874\n",
      "Sharpe: 0.813\n",
      "=================================\n",
      "day: 1258, episode: 33\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1875598.84\n",
      "total_reward: 875598.84\n",
      "total_cost: 225018.32\n",
      "total_trades: 34738\n",
      "Sharpe: 0.747\n",
      "=================================\n",
      "day: 1258, episode: 34\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2045210.68\n",
      "total_reward: 1045210.68\n",
      "total_cost: 228817.86\n",
      "total_trades: 34837\n",
      "Sharpe: 0.790\n",
      "=================================\n",
      "day: 1258, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1787753.79\n",
      "total_reward: 787753.79\n",
      "total_cost: 227000.73\n",
      "total_trades: 34718\n",
      "Sharpe: 0.666\n",
      "=================================\n",
      "day: 1258, episode: 36\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1642488.35\n",
      "total_reward: 642488.35\n",
      "total_cost: 214868.93\n",
      "total_trades: 34227\n",
      "Sharpe: 0.586\n",
      "=================================\n",
      "day: 1258, episode: 37\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1932217.84\n",
      "total_reward: 932217.84\n",
      "total_cost: 232795.79\n",
      "total_trades: 35061\n",
      "Sharpe: 0.765\n",
      "=================================\n",
      "day: 1258, episode: 38\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1990752.40\n",
      "total_reward: 990752.40\n",
      "total_cost: 231075.09\n",
      "total_trades: 34916\n",
      "Sharpe: 0.792\n",
      "=================================\n",
      "day: 1258, episode: 39\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1822105.64\n",
      "total_reward: 822105.64\n",
      "total_cost: 233099.14\n",
      "total_trades: 35277\n",
      "Sharpe: 0.720\n",
      "=================================\n",
      "day: 1258, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2085598.03\n",
      "total_reward: 1085598.03\n",
      "total_cost: 222727.56\n",
      "total_trades: 34655\n",
      "Sharpe: 0.812\n",
      "=================================\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 0.649    |\n",
      "| time/                 |          |\n",
      "|    fps                | 121      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 420      |\n",
      "|    total_timesteps    | 51200    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -6.47    |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.77     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.0374   |\n",
      "------------------------------------\n",
      "day: 1258, episode: 41\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1728561.47\n",
      "total_reward: 728561.47\n",
      "total_cost: 232803.86\n",
      "total_trades: 34966\n",
      "Sharpe: 0.654\n",
      "=================================\n",
      "day: 1258, episode: 42\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2344839.90\n",
      "total_reward: 1344839.90\n",
      "total_cost: 234916.41\n",
      "total_trades: 35034\n",
      "Sharpe: 0.939\n",
      "=================================\n",
      "day: 1258, episode: 43\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1834659.07\n",
      "total_reward: 834659.07\n",
      "total_cost: 230042.51\n",
      "total_trades: 34975\n",
      "Sharpe: 0.723\n",
      "=================================\n",
      "day: 1258, episode: 44\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1926495.99\n",
      "total_reward: 926495.99\n",
      "total_cost: 225970.84\n",
      "total_trades: 34863\n",
      "Sharpe: 0.770\n",
      "=================================\n",
      "day: 1258, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2269201.23\n",
      "total_reward: 1269201.23\n",
      "total_cost: 231703.67\n",
      "total_trades: 34959\n",
      "Sharpe: 0.918\n",
      "=================================\n",
      "day: 1258, episode: 46\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1964665.18\n",
      "total_reward: 964665.18\n",
      "total_cost: 234329.82\n",
      "total_trades: 35138\n",
      "Sharpe: 0.778\n",
      "=================================\n",
      "day: 1258, episode: 47\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1744296.27\n",
      "total_reward: 744296.27\n",
      "total_cost: 220290.01\n",
      "total_trades: 34656\n",
      "Sharpe: 0.647\n",
      "=================================\n",
      "day: 1258, episode: 48\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1848782.56\n",
      "total_reward: 848782.56\n",
      "total_cost: 224976.84\n",
      "total_trades: 34727\n",
      "Sharpe: 0.714\n",
      "=================================\n",
      "day: 1258, episode: 49\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1834887.20\n",
      "total_reward: 834887.20\n",
      "total_cost: 225468.55\n",
      "total_trades: 34672\n",
      "Sharpe: 0.699\n",
      "=================================\n",
      "day: 1258, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1670988.06\n",
      "total_reward: 670988.06\n",
      "total_cost: 229328.27\n",
      "total_trades: 34768\n",
      "Sharpe: 0.616\n",
      "=================================\n",
      "day: 1258, episode: 51\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1933372.78\n",
      "total_reward: 933372.78\n",
      "total_cost: 227650.58\n",
      "total_trades: 35094\n",
      "Sharpe: 0.767\n",
      "=================================\n",
      "day: 1258, episode: 52\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2122707.48\n",
      "total_reward: 1122707.48\n",
      "total_cost: 232555.92\n",
      "total_trades: 35079\n",
      "Sharpe: 0.855\n",
      "=================================\n",
      "day: 1258, episode: 53\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1938322.81\n",
      "total_reward: 938322.81\n",
      "total_cost: 222792.35\n",
      "total_trades: 34511\n",
      "Sharpe: 0.739\n",
      "=================================\n",
      "day: 1258, episode: 54\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1892497.12\n",
      "total_reward: 892497.12\n",
      "total_cost: 228225.23\n",
      "total_trades: 34914\n",
      "Sharpe: 0.743\n",
      "=================================\n",
      "day: 1258, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1955872.16\n",
      "total_reward: 955872.16\n",
      "total_cost: 223108.93\n",
      "total_trades: 34838\n",
      "Sharpe: 0.744\n",
      "=================================\n",
      "day: 1258, episode: 56\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1998221.25\n",
      "total_reward: 998221.25\n",
      "total_cost: 230948.62\n",
      "total_trades: 34907\n",
      "Sharpe: 0.805\n",
      "=================================\n",
      "day: 1258, episode: 57\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1901645.14\n",
      "total_reward: 901645.14\n",
      "total_cost: 228768.57\n",
      "total_trades: 34989\n",
      "Sharpe: 0.724\n",
      "=================================\n",
      "day: 1258, episode: 58\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1982465.52\n",
      "total_reward: 982465.52\n",
      "total_cost: 229519.25\n",
      "total_trades: 35046\n",
      "Sharpe: 0.767\n",
      "=================================\n",
      "day: 1258, episode: 59\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2117531.06\n",
      "total_reward: 1117531.06\n",
      "total_cost: 226660.11\n",
      "total_trades: 34838\n",
      "Sharpe: 0.852\n",
      "=================================\n",
      "day: 1258, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1832483.40\n",
      "total_reward: 832483.40\n",
      "total_cost: 227392.65\n",
      "total_trades: 34878\n",
      "Sharpe: 0.718\n",
      "=================================\n",
      "day: 1258, episode: 61\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1531066.15\n",
      "total_reward: 531066.15\n",
      "total_cost: 216637.34\n",
      "total_trades: 34320\n",
      "Sharpe: 0.520\n",
      "=================================\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 0.648    |\n",
      "| time/                 |          |\n",
      "|    fps                | 124      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 615      |\n",
      "|    total_timesteps    | 76800    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -1.1     |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 12.3     |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.186    |\n",
      "------------------------------------\n",
      "day: 1258, episode: 62\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1949118.84\n",
      "total_reward: 949118.84\n",
      "total_cost: 225260.16\n",
      "total_trades: 34639\n",
      "Sharpe: 0.773\n",
      "=================================\n",
      "day: 1258, episode: 63\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1849169.92\n",
      "total_reward: 849169.92\n",
      "total_cost: 215086.77\n",
      "total_trades: 34516\n",
      "Sharpe: 0.717\n",
      "=================================\n",
      "day: 1258, episode: 64\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1685148.15\n",
      "total_reward: 685148.15\n",
      "total_cost: 223354.18\n",
      "total_trades: 34623\n",
      "Sharpe: 0.612\n",
      "=================================\n",
      "day: 1258, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1966758.29\n",
      "total_reward: 966758.29\n",
      "total_cost: 233772.82\n",
      "total_trades: 35228\n",
      "Sharpe: 0.782\n",
      "=================================\n",
      "day: 1258, episode: 66\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1936034.92\n",
      "total_reward: 936034.92\n",
      "total_cost: 226188.49\n",
      "total_trades: 34783\n",
      "Sharpe: 0.750\n",
      "=================================\n",
      "day: 1258, episode: 67\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2197430.19\n",
      "total_reward: 1197430.19\n",
      "total_cost: 229370.28\n",
      "total_trades: 34952\n",
      "Sharpe: 0.924\n",
      "=================================\n",
      "day: 1258, episode: 68\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1929082.67\n",
      "total_reward: 929082.67\n",
      "total_cost: 215315.82\n",
      "total_trades: 34249\n",
      "Sharpe: 0.745\n",
      "=================================\n",
      "day: 1258, episode: 69\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1890291.30\n",
      "total_reward: 890291.30\n",
      "total_cost: 224090.03\n",
      "total_trades: 34610\n",
      "Sharpe: 0.739\n",
      "=================================\n",
      "day: 1258, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1983470.02\n",
      "total_reward: 983470.02\n",
      "total_cost: 227363.01\n",
      "total_trades: 34876\n",
      "Sharpe: 0.775\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "from finHRL.agent.models import baseRLAgent\n",
    "\n",
    "agent = baseRLAgent(env=tr_env)\n",
    "\n",
    "n_episodes = 70\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99\n",
    "max_grad_norm = 0.5\n",
    "n_steps = 256\n",
    "ent_coef = 0.0001\n",
    "\n",
    "model = agent.get_model(\"a2c\",\n",
    "                            learning_rate = learning_rate,\n",
    "                            gamma = gamma,\n",
    "                            max_grad_norm = max_grad_norm,\n",
    "                            n_steps = n_steps,\n",
    "                            ent_coef = ent_coef,\n",
    "                            verbose=1)\n",
    "\n",
    "\n",
    "trained_model = agent.train_model(\n",
    "    model,\n",
    "    tb_log_name=\"a2c_test1\",\n",
    "    total_timesteps= n_episodes*episode_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8473e869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n",
      "day: 250, episode: 2\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1037854.17\n",
      "total_reward: 37854.17\n",
      "total_cost: 1383.70\n",
      "total_trades: 3489\n",
      "Sharpe: 0.294\n",
      "=================================\n",
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "test_env = StockTradingEnv(\n",
    "    df = df_test,\n",
    "    stock_dim=stock_dimension,\n",
    "    hmax= 100,\n",
    "    initial_amount=1000000,\n",
    "    num_stock_shares=num_stock_shares,\n",
    "    buy_cost_pct=buy_cost_list,\n",
    "    sell_cost_pct=sell_cost_list,\n",
    "    state_space= state_space,\n",
    "    action_space= stock_dimension,\n",
    "    tech_indicator_list=INDICATORS,\n",
    "    make_plots=False,\n",
    "    print_verbosity=1\n",
    ")\n",
    "\n",
    "df_account_value_a2c, df_actions_a2c = baseRLAgent.predict_RL(\n",
    "    model=trained_model, \n",
    "    environment = test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd47957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-23 15:07:07,849] A new study created in memory with name: no-name-04e9bb65-f0b1-4361-a627-d597b99f341b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 87       |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 800      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.00408  |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -4.86    |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "day: 1258, episode: 1\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2039735.02\n",
      "total_reward: 1039735.02\n",
      "total_cost: 19155.59\n",
      "total_trades: 23473\n",
      "Sharpe: 0.923\n",
      "=================================\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.26e+03 |\n",
      "|    ep_rew_mean        | 0.711    |\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 1600     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.00408  |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.679   |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.000334 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-11-23 15:07:28,198] Trial 0 failed with parameters: {'gamma': 0.9825412090417985, 'max_grad_norm': 3.022986292922892, 'n_steps': 8, 'learning_rate': 0.004082650230163378, 'ent_coef': 5.716252431384161e-08} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/dev/hyperparameter_searching/base_RL_hs.py\", line 117, in objective\n",
      "    trained_model = agent.train_model(\n",
      "                    ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/finHRL/agent/models.py\", line 52, in train_model\n",
      "    model = model.learn(\n",
      "            ^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/a2c/a2c.py\", line 201, in learn\n",
      "    return super().learn(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 336, in learn\n",
      "    self.train()\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/a2c/a2c.py\", line 150, in train\n",
      "    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/policies.py\", line 737, in evaluate_actions\n",
      "    distribution = self._get_action_dist_from_latent(latent_pi)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/policies.py\", line 694, in _get_action_dist_from_latent\n",
      "    return self.action_dist.proba_distribution(mean_actions, self.log_std)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/distributions.py\", line 164, in proba_distribution\n",
      "    self.distribution = Normal(mean_actions, action_std)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/torch/distributions/normal.py\", line 56, in __init__\n",
      "    super().__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/Users/andoniiribarren/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/torch/distributions/distribution.py\", line 67, in __init__\n",
      "    if not valid.all():\n",
      "           ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-11-23 15:07:28,207] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdev\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhyperparameter_searching\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_RL_hs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hyperparams_opt_RL\n\u001b[32m      3\u001b[39m hs_opt = hyperparams_opt_RL(\n\u001b[32m      4\u001b[39m     df_train=df_train,\n\u001b[32m      5\u001b[39m     df_test=df_test,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     n_trials=\u001b[32m80\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mhs_opt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/dev/hyperparameter_searching/base_RL_hs.py:132\u001b[39m, in \u001b[36mhyperparams_opt_RL.run_opt\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_opt\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    131\u001b[39m     study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mprint\u001b[39m (\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBest hiperparams: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy.best_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    136\u001b[39m     t = study.best_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/optuna/study/study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/optuna/study/_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/optuna/study/_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/optuna/study/_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/optuna/study/_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/dev/hyperparameter_searching/base_RL_hs.py:117\u001b[39m, in \u001b[36mhyperparams_opt_RL.objective\u001b[39m\u001b[34m(self, trial)\u001b[39m\n\u001b[32m    106\u001b[39m agent = baseRLAgent(env=train_env)\n\u001b[32m    108\u001b[39m model = agent.get_model(\u001b[33m\"\u001b[39m\u001b[33ma2c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m                         learning_rate = learning_rate,\n\u001b[32m    110\u001b[39m                         gamma = gamma,\n\u001b[32m   (...)\u001b[39m\u001b[32m    113\u001b[39m                         ent_coef = ent_coef,\n\u001b[32m    114\u001b[39m                         verbose=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m trained_model = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ma2c_test1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_episodes_train\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepisode_len\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m df_account_value, _ = baseRLAgent.predict_RL(\n\u001b[32m    124\u001b[39m     model=trained_model, \n\u001b[32m    125\u001b[39m     environment = test_env)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df_account_value.account_value.iloc[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/finHRL/agent/models.py:52\u001b[39m, in \u001b[36mbaseRLAgent.train_model\u001b[39m\u001b[34m(model, tb_log_name, total_timesteps, callbacks)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model\u001b[39m(\n\u001b[32m     47\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     callbacks: \u001b[38;5;28mtype\u001b[39m[BaseCallback] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     51\u001b[39m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCallbackList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/a2c/a2c.py:201\u001b[39m, in \u001b[36mA2C.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[32m    194\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    200\u001b[39m ) -> SelfA2C:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28mself\u001b[39m._dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m callback.on_training_end()\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/a2c/a2c.py:150\u001b[39m, in \u001b[36mA2C.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_space, spaces.Discrete):\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# Convert discrete action from float to long\u001b[39;00m\n\u001b[32m    148\u001b[39m     actions = actions.long().flatten()\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m values, log_prob, entropy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m values = values.flatten()\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Normalize advantage (not present in the original implementation)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:737\u001b[39m, in \u001b[36mActorCriticPolicy.evaluate_actions\u001b[39m\u001b[34m(self, obs, actions)\u001b[39m\n\u001b[32m    735\u001b[39m     latent_pi = \u001b[38;5;28mself\u001b[39m.mlp_extractor.forward_actor(pi_features)\n\u001b[32m    736\u001b[39m     latent_vf = \u001b[38;5;28mself\u001b[39m.mlp_extractor.forward_critic(vf_features)\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m distribution = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m log_prob = distribution.log_prob(actions)\n\u001b[32m    739\u001b[39m values = \u001b[38;5;28mself\u001b[39m.value_net(latent_vf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/policies.py:694\u001b[39m, in \u001b[36mActorCriticPolicy._get_action_dist_from_latent\u001b[39m\u001b[34m(self, latent_pi)\u001b[39m\n\u001b[32m    691\u001b[39m mean_actions = \u001b[38;5;28mself\u001b[39m.action_net(latent_pi)\n\u001b[32m    693\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_dist, DiagGaussianDistribution):\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.action_dist, CategoricalDistribution):\n\u001b[32m    696\u001b[39m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.action_dist.proba_distribution(action_logits=mean_actions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/stable_baselines3/common/distributions.py:164\u001b[39m, in \u001b[36mDiagGaussianDistribution.proba_distribution\u001b[39m\u001b[34m(self, mean_actions, log_std)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[32m    158\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m \u001b[33;03m:return:\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m action_std = th.ones_like(mean_actions) * log_std.exp()\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28mself\u001b[39m.distribution = \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/torch/distributions/normal.py:56\u001b[39m, in \u001b[36mNormal.__init__\u001b[39m\u001b[34m(self, loc, scale, validate_args)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m     batch_shape = \u001b[38;5;28mself\u001b[39m.loc.size()\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TFM/repoTFM/HRLMAMLforST/hrlmamlenv/lib/python3.12/site-packages/torch/distributions/distribution.py:67\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     65\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[32m     66\u001b[39m         valid = constraint.check(value)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     68\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     69\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m             )\n\u001b[32m     75\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from dev.hyperparameter_searching.base_RL_hs import hyperparams_opt_RL\n",
    "\n",
    "hs_opt = hyperparams_opt_RL(\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    indicators=INDICATORS,\n",
    "    n_episodes_train=10,\n",
    "    n_trials=80\n",
    ")\n",
    "\n",
    "hs_opt.run_opt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111fcf7",
   "metadata": {},
   "source": [
    "# HRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a65d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finHRL.env_stocktrading.trading_env_HRL import StockTradingEnvHRL\n",
    "\n",
    "# state_space_manager = [close prices_i, MACD_i, rsi30_i, cci30_i, turbulences_i] Quizas quitar algn indicador\n",
    "# state_space_worker = [balance, close_prices_i, stock_shares_i, manager_actions_i]                 # QUizs aadir agn indicador de riesgo a estudiar y hacer pruebas\n",
    "# state_space_noHRL = [balance, close prices_i, stock_shares_i, MACD_i, rsi30_i, cci30_i, turbulences_i]\n",
    "\n",
    "# action_space_manager = {-1, 0, 1} * 30\n",
    "# action_space_manager = {0,1} * 30 (para calcular cuntas, multiplicar por hmax)\n",
    "\n",
    "episode_len = processed.dayorder.nunique()\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "\n",
    "state_space_manager = (len(INDICATORS) + 1)*stock_dimension\n",
    "state_space_worker = (1 + 3*stock_dimension)\n",
    "\n",
    "\n",
    "\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "\n",
    "tr_env = StockTradingEnvHRL(\n",
    "    df = processed,\n",
    "    stock_dim=stock_dimension,\n",
    "    hmax= 100,\n",
    "    initial_amount=1000000,\n",
    "    num_stock_shares=num_stock_shares,\n",
    "    buy_cost_pct=buy_cost_list,\n",
    "    sell_cost_pct=sell_cost_list,\n",
    "    state_space= state_space,\n",
    "    action_space= stock_dimension,\n",
    "    tech_indicator_list=INDICATORS,\n",
    "    make_plots=True,\n",
    "    print_verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530709f6",
   "metadata": {},
   "source": [
    "# Pendiente\n",
    "\n",
    "- LR est en 0.0003: Probar a bajar a 5e-5 o 1e-5\n",
    "- Incluir train -> Test etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrlmamlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
